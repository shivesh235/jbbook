Traceback (most recent call last):
  File "/home/sie/miniconda3/envs/env1/lib/python3.10/site-packages/jupyter_cache/executors/utils.py", line 58, in single_nb_execution
    executenb(
  File "/home/sie/miniconda3/envs/env1/lib/python3.10/site-packages/nbclient/client.py", line 1305, in execute
    return NotebookClient(nb=nb, resources=resources, km=km, **kwargs).execute()
  File "/home/sie/miniconda3/envs/env1/lib/python3.10/site-packages/jupyter_core/utils/__init__.py", line 165, in wrapped
    return loop.run_until_complete(inner)
  File "/home/sie/miniconda3/envs/env1/lib/python3.10/asyncio/base_events.py", line 649, in run_until_complete
    return future.result()
  File "/home/sie/miniconda3/envs/env1/lib/python3.10/site-packages/nbclient/client.py", line 705, in async_execute
    await self.async_execute_cell(
  File "/home/sie/miniconda3/envs/env1/lib/python3.10/site-packages/nbclient/client.py", line 1058, in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
  File "/home/sie/miniconda3/envs/env1/lib/python3.10/site-packages/nbclient/client.py", line 914, in _check_raise_for_error
    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
------------------
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
from torch.utils.data import DataLoader
from torchvision import transforms

torch.manual_seed(42)

# Hyperparameters
batch_size = 2048
latent_dim = 20 
learning_rate = 1e-3
num_epochs = 100
img_size = 28

transform = transforms.Compose([
    transforms.ToTensor(),
])

class VAE(nn.Module):
    def __init__(self, latent_dim):
        super(VAE, self).__init__()

        self.encoder = nn.Sequential(
            nn.Conv2d(1, 32, 3, 1), 
            nn.ReLU(),
            nn.Conv2d(32, 64, 3, 1),
            nn.MaxPool2d(2),
            nn.ReLU(),
            nn.Conv2d(64, 128, 3, 1),
            nn.ReLU(),
            nn.Conv2d(128, 256, 3, 1),
            nn.MaxPool2d(2),  # 256x4x4 output
            nn.ReLU(),
            nn.Flatten()
        )

        # Assuming the flattened encoder output is (256 * 4 * 4) = 4096
        self.fc_mu = nn.Linear(4096, latent_dim)
        self.fc_logvar = nn.Linear(4096, latent_dim)

        # Decoder
        self.fc_dec = nn.Linear(latent_dim, 4096)

        self.decoder = nn.Sequential(
            nn.Unflatten(1, (256, 4, 4)),
            nn.Upsample(scale_factor=2),
            nn.ReLU(),
            nn.ConvTranspose2d(256, 128, 3, 1),
            nn.ReLU(),
            nn.ConvTranspose2d(128, 64, 3, 1),
            nn.Upsample(scale_factor=2),
            nn.ReLU(),
            nn.ConvTranspose2d(64, 32, 3, 1),
            nn.ReLU(),
            nn.ConvTranspose2d(32, 1, 3, 1),
            nn.Sigmoid()                       
        )

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def forward(self, x):
        # Encoding
        encoded = self.encoder(x)
        mu = self.fc_mu(encoded)
        logvar = self.fc_logvar(encoded)

        z = self.reparameterize(mu, logvar)

        # Decoding
        decoded = self.fc_dec(z)
        decoded = self.decoder(decoded)
        return decoded, mu, logvar

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = VAE(latent_dim).to(device)
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

def loss_function(recon_x, x, mu, logvar):
    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum') / recon_x.size(0)
    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / recon_x.size(0)
    return BCE + KLD

# Load MNIST dataset
mnist_dataset = torchvision.datasets.MNIST(root='./mnist', train=True, transform=transform, download=True)
dataloader = DataLoader(mnist_dataset, batch_size=batch_size, shuffle=True)

# Training loop
for epoch in range(num_epochs):
    model.train()
    train_loss = 0
    for batch_idx, (data, _) in enumerate(dataloader):
        data = data.to(device)
        optimizer.zero_grad()
        recon_batch, mu, logvar = model(data)
        loss = loss_function(recon_batch, data, mu, logvar)
        loss.backward()
        train_loss += loss.item()
        optimizer.step()
    if epoch%20 == 0:
      print(f'Epoch {epoch+1}, Loss: {train_loss / len(dataloader.dataset):.4f}')
      # Save model
      torch.save(model.state_dict(), f'vae_mnist_{epoch}.pth')

# Generate samples
model.eval()
with torch.no_grad():
    sample = torch.randn(16, latent_dim).to(device)
    generated = model.decoder(model.fc_dec(sample)).cpu()

# Save generated images
torchvision.utils.save_image(generated, 'vae_mnist_samples.png', normalize=True, nrow=4)

print("Training complete. Generated samples saved as 'vae_mnist_samples.png'.")
------------------


[0;31m---------------------------------------------------------------------------[0m
[0;31mModuleNotFoundError[0m                       Traceback (most recent call last)
Cell [0;32mIn[1], line 4[0m
[1;32m      2[0m [38;5;28;01mimport[39;00m [38;5;21;01mtorch[39;00m[38;5;21;01m.[39;00m[38;5;21;01mnn[39;00m [38;5;28;01mas[39;00m [38;5;21;01mnn[39;00m
[1;32m      3[0m [38;5;28;01mimport[39;00m [38;5;21;01mtorch[39;00m[38;5;21;01m.[39;00m[38;5;21;01moptim[39;00m [38;5;28;01mas[39;00m [38;5;21;01moptim[39;00m
[0;32m----> 4[0m [38;5;28;01mimport[39;00m [38;5;21;01mtorchvision[39;00m
[1;32m      5[0m [38;5;28;01mfrom[39;00m [38;5;21;01mtorch[39;00m[38;5;21;01m.[39;00m[38;5;21;01mutils[39;00m[38;5;21;01m.[39;00m[38;5;21;01mdata[39;00m [38;5;28;01mimport[39;00m DataLoader
[1;32m      6[0m [38;5;28;01mfrom[39;00m [38;5;21;01mtorchvision[39;00m [38;5;28;01mimport[39;00m transforms

[0;31mModuleNotFoundError[0m: No module named 'torchvision'

